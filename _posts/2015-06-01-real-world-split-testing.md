---
layout: post
title:  "Real World Split Tesing"
date:   2015-06-01
description: What happens away from the statisitics text books
categories:
- ab
permalink: real-world-split-testing
---

##### Day 0
developer: "Ok, new site version is live and in split test"

##### Day 1
manager: "Hmm, conversion on alternative looks down"

statistician: "You can read to much into one day"

##### Day 2
manager: "Conversion is still down"

statistician: "We need to until our sample size of users has run"

##### Day 3
manager: "Ok, we are seeing a statistically significant drop on the alternative"

statistician: "Split testing is a well established protocol, we are not going to prematurely declare a winner because the testing process relies on known sample sizes. Test results can dip in and out of significance before the pre-determined number of visitors have seen the site."

developer: "Hmmm. Logged in users on IE 9 can't make a purchase on the new version"

Every stats, data and testing blog talks about correct ways to run an AB test, 
the stats to minimise Type I or Type II errors, calculating sample size. 

But development is hard. Testing is hard. Bugs happen.  The stats world feels oblivious.

